# Abstract
This is the rough research agenda that I am applying for funding for in late 2025.

We can break the outer alignment problem into roughly 4 easier steps:
1. Given a rigorous value specification, turn it into a mathematical property of a safer agent. 
2. Given a mathematical property, attempt to design a committee of simpler agents where the whole has the new property.
3. Analyze the committee using Game Theory to prove or disprove the claim that the committee has the new property.
4. Translate the committee into a single learning problem to prevent disassembly risks.

# Introduction
TO DO

# Big Picture:
The goal is to motivate breaking outer alignment into these 4 steps 
with the hope of tackling many subproblems within the space 
by showing that one to three outer alignment subproblems can be addressed.
Even if this single break into 4 steps doesn't generalize to other outer alignment subproblems,
the ideas for how to break the problem into a sequence of easier problems is worth motivating.
## Test Case: Mild Optimization
Mild optimization is a outer alignment subproblem
where we have at least 3 interesting angles on what the mathematical properties should be:
1. Satisficing agents
2. Sampling (or apprenticeship) agents
3. Quantilizing agents

## Test Case: Taskiness
Taskiness is an outer alignment subproblem 
where articulating the mathematical properties might be feasible.
1. Idea A: decrease the vingean uncertainty that a task is completed instead of decreasing the raw subjective utility.
This doesn't seem to address the time bounded nature of taskiness?

# Conclusion
TO DO
